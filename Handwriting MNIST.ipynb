{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Convolutional Neural Networks\n",
    "\n",
    "The main structural feature of RegularNets is that all the neurons are connected to each other. For example, when we have images with 28 by 28 pixels with only greyscale, we will end up having 784 (28 x 28 x 1) neurons in a layer which seems manageable. However, most images have way more pixels and they are not grey-scaled. Therefore, assuming that we have a set of color images in 4K Ultra HD, we will have 26,542,080 (4096 x 2160 x 3) different neurons connected to each other in the first layer which is not really manageable. Therefore, we can say that RegularNets are not scalable for image classification. However, especially when it comes to images, there seems to be little correlation or relation between two individual pixels unless they are close to each other. This leads to the idea of Convolutional Layers and Pooling Layers.\n",
    "Layers in a CNN\n",
    "\n",
    "We are capable of using many different layers in a convolutional neural network. However, convolution, pooling, and fully connect layers are the most important ones. Therefore, I will quickly introduce these layers before implementing them.\n",
    "Convolutional Layers\n",
    "\n",
    "Convolutional layer is the very first layer where we extract features from the images in our datasets. Due to the fact that pixels are only related with the adjacent and close pixels, convolution allows us to preserve the relationship between different parts of an image. Convolution is basically filtering the image with a smaller pixel filter to decrease the size of the image without loosing the relationship between pixels. When we apply convolution to 5x5 image by using a 3x3 filter with 1x1 stride (1 pixel shift at each step). We will end up having a 3x3 output (64% decrease in complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling Layer\n",
    "\n",
    "When constructing CNNs, it is common to insert pooling layers after each convolution layer to reduce the spatial size of the representation to reduce the parameter counts which reduces the computational complexity. In addition, pooling layers also helps with the overfitting problem. Basically we select a pooling size to reduce the amount of the parameters by selecting the maximum, average, or sum values inside these pixels. Max Pooling, one of the most common pooling techniques, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Set of Fully Connected Layers\n",
    "\n",
    "A fully connected network is our RegularNet where each parameter is linked to one another to determine the true relation and effect of each parameter on the labels. Since our time-space complexity is vastly reduced thanks to convolution and pooling layers, we can construct a fully connected network in the end to classify our images. A set of fully connected layers looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some idea about the individual layers that we will use, I think it is time to share an overview look of a complete convolutional neural network.\n",
    "A Convolutional Neural Network Example [3]\n",
    "\n",
    "And now that you have an idea of convolutional neural network that you can build for image classification, we can get the most cliche dataset for classification: MNIST dataset, which stands for Modified National Institute of Standards and Technology database. It is a large database of handwritten digits that is commonly used for training various image processing system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the Mnist Data\n",
    "\n",
    "The MNIST dataset is one of the most common datasets used for image classification and accessible from many different sources. In fact, even Tensorflow and Keras allow us to import and download the MNIST dataset directly from their API. Therefore, I will start with the following two lines to import tensorflow and MNIST dataset under the Keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST database contains 60,000 training images and 10,000 testing images taken from American Census Bureau employees and American high school students [4]. Therefore, in the second line, I have separated these two groups as train and test and also separated the labels and the images. x_train and x_test parts contain greyscale RGB codes (from 0 to 255) while y_train and y_test parts contains labels from 0 to 9 which represents which number they actually are. To visualize these numbers, we can get help from matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dd7ce67ac8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "image_index = 10\n",
    "print(y_train[image_index]) # The label is 8\n",
    "plt.imshow(x_train[image_index], cmap='Greys')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will get (60000, 28, 28). As you might have guessed 60000 represents the number of images in the train dataset and (28, 28) represents the size of the image: 28 x 28 pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping and Normalizing the Images\n",
    "\n",
    "To be able to use the dataset in Keras API, we need 4-dims numpy arrays. However, as we see above, our array is 3-dims. In addition, we must normalize our data as it is always required in neural network models. We can achieve this by dividing the RGB codes to 255 (which is the maximum RGB code minus the minimum RGB code). \n",
    "\n",
    "'1' represents greyscale value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "# Making sure that the values are float so that we can get decimal points after division\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Convolutional Neural Network\n",
    "\n",
    "We will build our model by using high level Keras API which uses either TensorFlow or Theano on the backend. I would like to mention that there are several high level TensorFlow APIs such as Layers, Keras, and Estimators which helps us create neural networks with high level knowledge. However, this may lead to confusion since they all varies in their implementation structure. Therefore, if you see completely different codes for the same neural network although they all use tensorflow, this is why. I will use the most straightforward API which is Keras. Therefore, I will import the Sequential Model from Keras and add Conv2D, MaxPooling, Flatten, Dropout, and Dense layers. I have already talked about Conv2D, Maxpooling, and Dense layers. In addition, Dropout layers fight with the overfitting by disregarding some of the neurons while training while Flatten layers flatten 2D arrays to 1D array before building the fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0905 21:57:12.906712 24820 deprecation_wrapper.py:119] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0905 21:57:12.919679 24820 deprecation_wrapper.py:119] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0905 21:57:12.921638 24820 deprecation_wrapper.py:119] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0905 21:57:12.936598 24820 deprecation_wrapper.py:119] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0905 21:57:12.958543 24820 deprecation_wrapper.py:119] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0905 21:57:12.966520 24820 deprecation.py:506] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "# Creating a Sequential Model and adding the layers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "model.add(Dense(128, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may experiment with any number for the first Dense layer; however, the final Dense layer must have 10 neurons since we have 10 number classes (0, 1, 2, …, 9). You may always experiment with kernel size, pool size, activation functions, dropout rate, and number of neurons in the first Dense layer to get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling and Fitting the Model\n",
    "\n",
    "With the above code, we created an non-optimized empty CNN. Now it is time to set an optimizer with a given loss function which uses a metric. Then, we can fit the model by using our train data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0905 21:57:39.165476 24820 deprecation_wrapper.py:119] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0905 21:57:39.182430 24820 deprecation_wrapper.py:119] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0905 21:57:39.269199 24820 deprecation.py:323] From C:\\Users\\shrey\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 30s 497us/step - loss: 0.2026 - acc: 0.94012s -  - ETA: 0s - loss: 0.20\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 29s 484us/step - loss: 0.0813 - acc: 0.9750\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 29s 488us/step - loss: 0.0578 - acc: 0.9820\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 28s 468us/step - loss: 0.0437 - acc: 0.9862\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 29s 478us/step - loss: 0.0352 - acc: 0.9886\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 29s 485us/step - loss: 0.0288 - acc: 0.9906\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 29s 485us/step - loss: 0.0243 - acc: 0.9918\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 30s 496us/step - loss: 0.0214 - acc: 0.9924\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 30s 500us/step - loss: 0.0192 - acc: 0.9932\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 29s 486us/step - loss: 0.0183 - acc: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dd7cea8a90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(x=x_train,y=y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment with the optimizer, loss function, metrics, and epochs. However, I can say that adam optimizer is usually out-performs the other optimizers. I am not sure if you can actually change the loss function for multi-class classification. Feel free to experiment and comment below. Epoch number might seem a bit small. However, you will reach to 98–99% test accuracy. Since the MNIST dataset does not require heavy computing power, you may easily experiment with the epoch number as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the Model\n",
    "\n",
    "Finally, you may evaluate the trained model with x_test and y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 131us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05256365565882879, 0.9863]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved 98.6% accuracy with such basic model. To be frank, in many image classification cases (e.g. for autonomous cars), we cannot even tolerate 0.1% error since, as an analogy, it will cause 1 accident in 1000 cases. However, for our first model, I would say the result is still pretty good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1dd7b375c88>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 3213\n",
    "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))# MNIST has array of size 784\n",
    "print(pred.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
